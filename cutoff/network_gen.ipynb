{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "db_file = 'uniref90-10pm.db'\n",
    "threshold = 0.77\n",
    "batch_size = 1_000_000  # Number of pairs per batch\n",
    "checkpoint_file = 'checkpoint.txt'\n",
    "batch_prefix = 'protein_similarity_batch_'\n",
    "current_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log messages with timestamps\n",
    "def log(message):\n",
    "    print(f\"[{datetime.now()}] {message}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in TSV format\n",
    "def save_batch_in_tsv(filename, results):\n",
    "    with open(filename, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(f\"{result['Protein_A']}\\t{result['Protein_B']}\\t{result['Cosine_Similarity']:.6f}\\n\")\n",
    "    log(f\"Batch saved in TSV format: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:31:53.703163] Resuming from i=100, j=100, batch=1\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint if it exists\n",
    "start_i, start_j = 0, 1\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        start_i, start_j, current_batch = map(int, f.readline().strip().split())\n",
    "    log(f\"Resuming from i={start_i}, j={start_j}, batch={current_batch}\")\n",
    "else:\n",
    "    log(\"Starting from the beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:31:53.719280] Connecting to the database...\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "log(\"Connecting to the database...\")\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:31:55.436364] Executing SQL query to retrieve protein data...\n",
      "[2024-11-16 14:42:21.262315] Retrieved 5190657 protein entries from the database\n"
     ]
    }
   ],
   "source": [
    "# Query to select proteins with sequence length <= 1200 and non-null embeddings\n",
    "log(\"Executing SQL query to retrieve protein data...\")\n",
    "query = \"\"\"\n",
    "SELECT name, embedding \n",
    "FROM uniref90 \n",
    "WHERE LENGTH(sequence) <= 1200 AND embedding IS NOT NULL\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "log(f\"Retrieved {len(rows)} protein entries from the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:50:37.719318] Processing embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Process embeddings and ensure shape is 1024\n",
    "log(\"Processing embeddings...\")\n",
    "protein_ids = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:52:31.237098] Processed 5190657 embeddings with shape (1024,)\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    protein_id, embedding_blob = row\n",
    "    \n",
    "    # Convert BLOB to NumPy array\n",
    "    embedding = np.frombuffer(embedding_blob, dtype=np.float16)\n",
    "    \n",
    "    # Ensure shape is 1024\n",
    "    if embedding.shape[0] != 1024:\n",
    "        embedding = np.frombuffer(embedding_blob, dtype=np.float32)\n",
    "    if embedding.shape[0] < 1024:\n",
    "        embedding = np.pad(embedding, (0, 1024 - embedding.shape[0]), mode='constant')\n",
    "    elif embedding.shape[0] > 1024:\n",
    "        embedding = embedding[:1024]\n",
    "    \n",
    "    protein_ids.append(protein_id)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "log(f\"Processed {len(embeddings)} embeddings with shape (1024,)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:56:16.774293] Converting embeddings to GPU...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert embeddings to CuPy array\u001b[39;00m\n\u001b[0;32m      2\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting embeddings to GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\BROMBERG\\CODE\\code\\Lib\\site-packages\\cupy\\_creation\\from_data.py:53\u001b[0m, in \u001b[0;36marray\u001b[1;34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray\u001b[39m(obj, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m      8\u001b[0m           blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an array on the current device.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    This function currently does not support the ``subok`` option.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2408\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2431\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2490\u001b[0m, in \u001b[0;36mcupy._core.core._array_from_nested_sequence\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2534\u001b[0m, in \u001b[0;36mcupy._core.core._array_from_nested_numpy_sequence\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
     ]
    }
   ],
   "source": [
    "# Convert embeddings to CuPy array\n",
    "log(\"Converting embeddings to GPU...\")\n",
    "embeddings = cp.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.391503] Normalizing embeddings...\n",
      "[2024-11-16 14:04:08.391503] Computing pairwise cosine similarity matrix on the GPU...\n",
      "[2024-11-16 14:04:08.391503] Cosine similarity matrix computation complete\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "log(\"Normalizing embeddings...\")\n",
    "norms = cp.linalg.norm(embeddings, axis=1, keepdims=True)  # Compute L2 norms\n",
    "normalized_embeddings = embeddings / norms  # Normalize each embedding\n",
    "\n",
    "# Compute pairwise cosine similarity matrix\n",
    "log(\"Computing pairwise cosine similarity matrix on the GPU...\")\n",
    "similarity_matrix = cp.matmul(normalized_embeddings, normalized_embeddings.T)\n",
    "similarity_matrix = cp.asnumpy(similarity_matrix)  # Convert back to NumPy for final processing\n",
    "log(\"Cosine similarity matrix computation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for pairs that meet the threshold\n",
    "results = []\n",
    "\n",
    "# Main processing loop with resumption capability\n",
    "num_proteins = len(protein_ids)\n",
    "for i in range(start_i, num_proteins):\n",
    "    for j in range(i + 1 if i != start_i else start_j, num_proteins):\n",
    "        similarity_score = similarity_matrix[i, j]\n",
    "        if similarity_score >= threshold:\n",
    "            results.append({\n",
    "                'Protein_A': protein_ids[i],\n",
    "                'Protein_B': protein_ids[j],\n",
    "                'Cosine_Similarity': similarity_score\n",
    "            })\n",
    "\n",
    "        # If batch is full, save it and clear results\n",
    "        if len(results) >= batch_size:\n",
    "            batch_filename = f'{batch_prefix}{current_batch}.tsv'\n",
    "            save_batch_in_tsv(batch_filename, results)\n",
    "            current_batch += 1\n",
    "            results = []  # Clear the results for the next batch\n",
    "\n",
    "            # Update the checkpoint file\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                f.write(f\"{i} {j} {current_batch}\\n\")\n",
    "            log(f\"Checkpoint updated: i={i}, j={j}, batch={current_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.439207] Batch saved in TSV format: protein_similarity_batch_0.tsv\n",
      "[2024-11-16 14:04:08.439207] Final checkpoint updated to indicate completion\n",
      "[2024-11-16 14:04:08.439207] All pairwise similarity scores (≥ 0.4) saved in TSV format.\n"
     ]
    }
   ],
   "source": [
    "# Save any remaining results in the final batch\n",
    "if results:\n",
    "    batch_filename = f'{batch_prefix}{current_batch}.tsv'\n",
    "    save_batch_in_tsv(batch_filename, results)\n",
    "\n",
    "    # Update the checkpoint file for the final state\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        f.write(f\"{num_proteins} {num_proteins} {current_batch + 1}\\n\")\n",
    "    log(\"Final checkpoint updated to indicate completion\")\n",
    "\n",
    "log(f\"All pairwise similarity scores (≥ {threshold}) saved in TSV format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
