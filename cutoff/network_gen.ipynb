{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "db_file = 'uniref90-10pm.db'\n",
    "threshold = 0.4\n",
    "batch_size = 1_000_000  # Number of pairs per batch\n",
    "checkpoint_file = 'checkpoint.txt'\n",
    "batch_prefix = 'protein_similarity_batch_'\n",
    "current_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log messages with timestamps\n",
    "def log(message):\n",
    "    print(f\"[{datetime.now()}] {message}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in TSV format\n",
    "def save_batch_in_tsv(filename, results):\n",
    "    with open(filename, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(f\"{result['Protein_A']}\\t{result['Protein_B']}\\t{result['Cosine_Similarity']:.6f}\\n\")\n",
    "    log(f\"Batch saved in TSV format: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.310927] Starting from the beginning\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint if it exists\n",
    "start_i, start_j = 0, 1\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        start_i, start_j, current_batch = map(int, f.readline().strip().split())\n",
    "    log(f\"Resuming from i={start_i}, j={start_j}, batch={current_batch}\")\n",
    "else:\n",
    "    log(\"Starting from the beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.319244] Connecting to the database...\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "log(\"Connecting to the database...\")\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.335277] Executing SQL query to retrieve protein data...\n",
      "[2024-11-16 14:04:08.335277] Retrieved 100 protein entries from the database\n"
     ]
    }
   ],
   "source": [
    "# Query to select proteins with sequence length <= 1200 and non-null embeddings\n",
    "log(\"Executing SQL query to retrieve protein data...\")\n",
    "query = \"\"\"\n",
    "SELECT name, embedding \n",
    "FROM uniref90 \n",
    "WHERE LENGTH(sequence) <= 1200 AND embedding IS NOT NULL\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "log(f\"Retrieved {len(rows)} protein entries from the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.351508] Processing embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Process embeddings and ensure shape is 1024\n",
    "log(\"Processing embeddings...\")\n",
    "protein_ids = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.359677] Processed 100 embeddings with shape (1024,)\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    protein_id, embedding_blob = row\n",
    "    \n",
    "    # Convert BLOB to NumPy array\n",
    "    embedding = np.frombuffer(embedding_blob, dtype=np.float16)\n",
    "    \n",
    "    # Ensure shape is 1024\n",
    "    if embedding.shape[0] != 1024:\n",
    "        embedding = np.frombuffer(embedding_blob, dtype=np.float32)\n",
    "    if embedding.shape[0] < 1024:\n",
    "        embedding = np.pad(embedding, (0, 1024 - embedding.shape[0]), mode='constant')\n",
    "    elif embedding.shape[0] > 1024:\n",
    "        embedding = embedding[:1024]\n",
    "    \n",
    "    protein_ids.append(protein_id)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "log(f\"Processed {len(embeddings)} embeddings with shape (1024,)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.375692] Converting embeddings to GPU...\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to CuPy array\n",
    "log(\"Converting embeddings to GPU...\")\n",
    "embeddings = cp.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.391503] Normalizing embeddings...\n",
      "[2024-11-16 14:04:08.391503] Computing pairwise cosine similarity matrix on the GPU...\n",
      "[2024-11-16 14:04:08.391503] Cosine similarity matrix computation complete\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "log(\"Normalizing embeddings...\")\n",
    "norms = cp.linalg.norm(embeddings, axis=1, keepdims=True)  # Compute L2 norms\n",
    "normalized_embeddings = embeddings / norms  # Normalize each embedding\n",
    "\n",
    "# Compute pairwise cosine similarity matrix\n",
    "log(\"Computing pairwise cosine similarity matrix on the GPU...\")\n",
    "similarity_matrix = cp.matmul(normalized_embeddings, normalized_embeddings.T)\n",
    "similarity_matrix = cp.asnumpy(similarity_matrix)  # Convert back to NumPy for final processing\n",
    "log(\"Cosine similarity matrix computation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for pairs that meet the threshold\n",
    "results = []\n",
    "\n",
    "# Main processing loop with resumption capability\n",
    "num_proteins = len(protein_ids)\n",
    "for i in range(start_i, num_proteins):\n",
    "    for j in range(i + 1 if i != start_i else start_j, num_proteins):\n",
    "        similarity_score = similarity_matrix[i, j]\n",
    "        if similarity_score >= threshold:\n",
    "            results.append({\n",
    "                'Protein_A': protein_ids[i],\n",
    "                'Protein_B': protein_ids[j],\n",
    "                'Cosine_Similarity': similarity_score\n",
    "            })\n",
    "\n",
    "        # If batch is full, save it and clear results\n",
    "        if len(results) >= batch_size:\n",
    "            batch_filename = f'{batch_prefix}{current_batch}.tsv'\n",
    "            save_batch_in_tsv(batch_filename, results)\n",
    "            current_batch += 1\n",
    "            results = []  # Clear the results for the next batch\n",
    "\n",
    "            # Update the checkpoint file\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                f.write(f\"{i} {j} {current_batch}\\n\")\n",
    "            log(f\"Checkpoint updated: i={i}, j={j}, batch={current_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 14:04:08.439207] Batch saved in TSV format: protein_similarity_batch_0.tsv\n",
      "[2024-11-16 14:04:08.439207] Final checkpoint updated to indicate completion\n",
      "[2024-11-16 14:04:08.439207] All pairwise similarity scores (≥ 0.4) saved in TSV format.\n"
     ]
    }
   ],
   "source": [
    "# Save any remaining results in the final batch\n",
    "if results:\n",
    "    batch_filename = f'{batch_prefix}{current_batch}.tsv'\n",
    "    save_batch_in_tsv(batch_filename, results)\n",
    "\n",
    "    # Update the checkpoint file for the final state\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        f.write(f\"{num_proteins} {num_proteins} {current_batch + 1}\\n\")\n",
    "    log(\"Final checkpoint updated to indicate completion\")\n",
    "\n",
    "log(f\"All pairwise similarity scores (≥ {threshold}) saved in TSV format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
